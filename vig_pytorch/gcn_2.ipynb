{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2623, 120, 120, 3)\n"
     ]
    }
   ],
   "source": [
    "# load image from npy file\n",
    "images = np.load('tranformed_cavallo.npy')\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_image(image, dim):\n",
    "    parts = []\n",
    "    height, width, _ = image.shape\n",
    "    subpart_width = width // dim\n",
    "    subpart_height = height // dim\n",
    "    \n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            part = image[i*subpart_height:(i+1)*subpart_height, j*subpart_width:(j+1)*subpart_width]\n",
    "            parts.append(part)\n",
    "    \n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combinations(parts, num_combinations):\n",
    "    combinations = []\n",
    "    original_positions = []\n",
    "    indices = list(range(len(parts)))\n",
    "    \n",
    "    for _ in range(num_combinations):\n",
    "        temp = []\n",
    "        random.shuffle(indices)\n",
    "        combination = [parts[i] for i in indices]\n",
    "        #further divide each part into 4x4 subpart\n",
    "        for i in range(len(combination)):\n",
    "            temp += divide_image(combination[i], 4)\n",
    "\n",
    "        combinations.append(temp)\n",
    "        original_positions.append(indices.copy())\n",
    "    \n",
    "    return combinations, original_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parts = divide_image(images[1], 3)\n",
    "# combinations, original_positions = generate_combinations(parts, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_image(image):\n",
    "    subparts = []\n",
    "    height, width, _ = image.shape\n",
    "    subpart_width = width // 3\n",
    "    subpart_height = height // 3\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            part = image[i*subpart_height:(i+1)*subpart_height, j*subpart_width:(j+1)*subpart_width]\n",
    "            for k in range(4):\n",
    "                for l in range(4):\n",
    "                    subpart = part[k*subpart_height//4:(k+1)*subpart_height//4, l*subpart_width//4:(l+1)*subpart_width//4]\n",
    "                    subparts.append(subpart)\n",
    "    \n",
    "    return subparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(subparts):\n",
    "    # Define the number of nodes and edges\n",
    "    num_nodes = 144\n",
    "    internal_edges = []  # List to store internal edges within each 4x4 grid\n",
    "    part_adjacent_edges = []  # List to store edges between adjacent parts\n",
    "    \n",
    "    # Define internal edges between consecutive subparts in each 4x4 grid (tile)\n",
    "    for part_idx in range(9):  # There are 9 parts in the 3x3 grid\n",
    "        # Calculate the starting index for the current part\n",
    "        start_idx = part_idx * 16\n",
    "\n",
    "        # Connect each node in the 4x4 grid\n",
    "        for row in range(4):\n",
    "            for col in range(4):\n",
    "                current_index = start_idx + row * 4 + col\n",
    "\n",
    "                # Connect to the right neighbor (if exists)\n",
    "                if col < 3:\n",
    "                    right_neighbor_index = current_index + 1\n",
    "                    internal_edges.append((current_index, right_neighbor_index))\n",
    "                    internal_edges.append((right_neighbor_index, current_index))\n",
    "\n",
    "                # Connect to the bottom neighbor (if exists)\n",
    "                if row < 3:\n",
    "                    bottom_neighbor_index = current_index + 4\n",
    "                    internal_edges.append((current_index, bottom_neighbor_index))\n",
    "                    internal_edges.append((bottom_neighbor_index, current_index))\n",
    "\n",
    "    # Define edges between adjacent parts (horizontally and vertically)\n",
    "    # Horizontal connections\n",
    "    for part_row in range(3):  # 3 rows of parts\n",
    "        for part_col in range(2):  # 2 columns of adjacent parts\n",
    "            # Calculate the start index of each adjacent part\n",
    "            part1_start_idx = (part_row * 3 + part_col) * 16\n",
    "            part2_start_idx = (part_row * 3 + part_col + 1) * 16\n",
    "            \n",
    "            # Connect each node in the rightmost column of part1 with the leftmost column of part2\n",
    "            for row in range(4):\n",
    "                part1_node_index = part1_start_idx + row * 4 + 3  # Rightmost column in part1\n",
    "                part2_node_index = part2_start_idx + row * 4  # Leftmost column in part2\n",
    "                \n",
    "                part_adjacent_edges.append((part1_node_index, part2_node_index))\n",
    "                part_adjacent_edges.append((part2_node_index, part1_node_index))\n",
    "\n",
    "    # Vertical connections\n",
    "    for part_col in range(3):  # 3 columns of parts\n",
    "        for part_row in range(2):  # 2 rows of adjacent parts\n",
    "            # Calculate the start index of each adjacent part\n",
    "            part1_start_idx = (part_row * 3 + part_col) * 16\n",
    "            part2_start_idx = ((part_row + 1) * 3 + part_col) * 16\n",
    "            \n",
    "            # Connect each node in the bottommost row of part1 with the topmost row of part2\n",
    "            for col in range(4):\n",
    "                part1_node_index = part1_start_idx + (3 * 4) + col  # Bottommost row in part1\n",
    "                part2_node_index = part2_start_idx + col  # Topmost row in part2\n",
    "                \n",
    "                part_adjacent_edges.append((part1_node_index, part2_node_index))\n",
    "                part_adjacent_edges.append((part2_node_index, part1_node_index))\n",
    "\n",
    "    # Combine internal edges and part adjacent edges into one edge list\n",
    "    # Create an attribute list to label each edge type\n",
    "    edges = internal_edges + part_adjacent_edges\n",
    "    edge_type = [0] * len(internal_edges) + [1] * len(part_adjacent_edges)  # 0 for internal, 1 for adjacent\n",
    "    \n",
    "    # Convert edges list to a tensor and transpose it\n",
    "    edge_index = torch.tensor(edges).T\n",
    "    edge_attr = torch.tensor(edge_type)  # Attribute tensor for edge types\n",
    "\n",
    "    # Convert subparts list to a tensor of shape `(num_nodes, C, H, W)`\n",
    "    # Assuming each subpart is a tensor with shape (C, H, W)\n",
    "    subparts = [torch.tensor(subpart).view(-1) for subpart in subparts]\n",
    "    node_features = torch.stack(subparts)  # Stack list of tensors into one tensor\n",
    "\n",
    "    # Create the Data object for PyTorch Geometric\n",
    "    graph_data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(imgs):\n",
    "    for i in imgs:\n",
    "        parts = divide_image(i, 3)\n",
    "        combinations, _ = generate_combinations(parts, 1)\n",
    "        for comb in combinations:\n",
    "            graph_data = create_graph(comb)\n",
    "            yield graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(\n",
    "            dim=-1\n",
    "        )  # product of a pair of nodes on each edge\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_adjacent_edges(graph_data):\n",
    "    # Access edge_index and edge_attr tensors\n",
    "    edge_index = graph_data.edge_index\n",
    "    edge_attr = graph_data.edge_attr\n",
    "\n",
    "    # Lists to store the indices of edges with attribute `1`\n",
    "    adjacent_edge_indices = []\n",
    "\n",
    "    # Iterate through each edge\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        # Check the edge attribute\n",
    "        if edge_attr[i].item() == 1:\n",
    "            adjacent_edge_indices.append(i)\n",
    "\n",
    "    # Filter edge_index and edge_attr tensors to include only edges with attribute `1`\n",
    "    filtered_edge_index = edge_index[:, adjacent_edge_indices]\n",
    "    filtered_edge_attr = edge_attr[adjacent_edge_indices]\n",
    "\n",
    "    # all idx other tham filtered_edge_index\n",
    "    all_idx = list(range(edge_index.shape[1]))\n",
    "    for idx in adjacent_edge_indices:\n",
    "        all_idx.remove(idx)\n",
    "\n",
    "    graph_data_edge_index = edge_index[:, all_idx]\n",
    "    graph_data_edge_attr = edge_attr[all_idx]\n",
    "\n",
    "    graph_data = Data(\n",
    "        x=graph_data.x,\n",
    "        edge_index=graph_data_edge_index,\n",
    "        edge_attr= graph_data_edge_attr,\n",
    "    )\n",
    "\n",
    "    graph_data_adjacent = Data(\n",
    "        x=graph_data.x,\n",
    "        edge_index=filtered_edge_index,\n",
    "        edge_attr=filtered_edge_attr\n",
    "    )\n",
    "    \n",
    "    return graph_data, graph_data_adjacent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_edges(graph_data, split_data):\n",
    "#     \"\"\"\n",
    "#     Combines internal edges from `graph_data` with the edges from `split_data`.\n",
    "#     \"\"\"\n",
    "#     # Get internal edges from the original graph data\n",
    "#     internal_edge_index = graph_data.edge_index\n",
    "#     internal_edge_attr = graph_data.edge_attr\n",
    "\n",
    "#     # Get edges and attributes from the split data (train/val/test)\n",
    "#     split_edge_index = split_data.edge_index\n",
    "#     split_edge_attr = split_data.edge_attr\n",
    "\n",
    "#     # Combine edge indices and attributes\n",
    "#     combined_edge_index = torch.cat((internal_edge_index, split_edge_index), dim=1)\n",
    "#     combined_edge_attr = torch.cat((internal_edge_attr, split_edge_attr))\n",
    "\n",
    "#     # Create a new Data object with the combined edges\n",
    "#     combined_data = Data(\n",
    "#         x=graph_data.x,  # Use the same node features\n",
    "#         num_nodes = graph_data.x.shape[0],\n",
    "#         edge_index=combined_edge_index,\n",
    "#         edge_attr=combined_edge_attr\n",
    "#     )\n",
    "\n",
    "#     return combined_data\n",
    "\n",
    "def combine_edges(graph_data, split_data):\n",
    "    \"\"\"\n",
    "    Combines internal edges from `graph_data` with the edges from `split_data`, including labels.\n",
    "    \"\"\"\n",
    "    # Combine edge indices and attributes\n",
    "    combined_edge_index = torch.cat((graph_data.edge_index, split_data.edge_index), dim=1)\n",
    "    combined_edge_attr = torch.cat((graph_data.edge_attr, split_data.edge_attr))\n",
    "\n",
    "    # Check if edge_label and edge_label_index are present and combine them if they are\n",
    "    if hasattr(graph_data, 'edge_label') and hasattr(split_data, 'edge_label'):\n",
    "        combined_edge_label = torch.cat((graph_data.edge_label, split_data.edge_label))\n",
    "        combined_edge_label_index = torch.cat((graph_data.edge_label_index, split_data.edge_label_index),dim=1)\n",
    "\n",
    "    # Create a new Data object\n",
    "    combined_data = Data(\n",
    "        x=graph_data.x,  # Use the same node features\n",
    "        num_nodes = graph_data.x.shape[0],\n",
    "        edge_index=combined_edge_index,\n",
    "        edge_attr=combined_edge_attr,\n",
    "        edge_label=combined_edge_label if 'combined_edge_label' in locals() else None,\n",
    "        edge_label_index=combined_edge_label_index if 'combined_edge_label_index' in locals() else None\n",
    "    )\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_link_predictor(\n",
    "#     model, train_loader, val_loader, optimizer, criterion, n_epochs=100\n",
    "# ):\n",
    "\n",
    "#     for epoch in range(1, n_epochs + 1):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "\n",
    "#         # Iterate through the DataLoader\n",
    "#         for batch in train_loader:\n",
    "#             # Zero out the gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # batch.edge_index = batch.edge_index.T\n",
    "\n",
    "#             # Encode the batch's node features and edge indices\n",
    "#             z = model.encode(batch.x.float(), batch.edge_index)\n",
    "\n",
    "#             # Sample negative edges\n",
    "#             neg_edge_index = negative_sampling(\n",
    "#                 edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "#                 num_neg_samples=batch.edge_label_index.size(1),\n",
    "#                 method='sparse'\n",
    "#             )\n",
    "\n",
    "#             # Combine positive and negative edge indices and labels\n",
    "#             edge_label_index = torch.cat(\n",
    "#                 [batch.edge_label_index, neg_edge_index],\n",
    "#                 dim=-1,\n",
    "#             )\n",
    "#             edge_label = torch.cat([\n",
    "#                 batch.edge_label,\n",
    "#                 batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "#             ], dim=0)\n",
    "\n",
    "#             # Decode and compute the loss\n",
    "#             out = model.decode(z, edge_label_index).view(-1)\n",
    "#             loss = criterion(out, edge_label)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             print(\"hi\")\n",
    "\n",
    "#         # Compute the average training loss for the epoch\n",
    "#         avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "#         # Evaluate the model on validation data\n",
    "#         val_auc = eval_link_predictor(model, val_loader)\n",
    "\n",
    "#         if epoch % 10 == 0:\n",
    "#             print(f\"Epoch: {epoch:03d}, Avg Train Loss: {avg_train_loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[144, 300], edge_index=[2, 36], edge_attr=[36], edge_label=[30], edge_label_index=[2, 30])\n",
      "Data(x=[144, 300], edge_index=[2, 432], edge_attr=[432], edge_label=[0], edge_label_index=[2, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "\n",
    "    parts = divide_image(images[i])\n",
    "\n",
    "    graph_data = create_graph(parts)\n",
    "\n",
    "\n",
    "    graph_data, graph_data_adjacent = filter_adjacent_edges(graph_data)\n",
    "\n",
    "    split = T.RandomLinkSplit(\n",
    "        num_val=0.33,\n",
    "        num_test=0.33,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=True,\n",
    "        neg_sampling_ratio=1.0,\n",
    "    )\n",
    "    split_2 = T.RandomLinkSplit(\n",
    "        num_val=0,\n",
    "        num_test=0,\n",
    "        is_undirected=True,\n",
    "        add_negative_train_samples=True,\n",
    "        neg_sampling_ratio=1.0,\n",
    "    )\n",
    "\n",
    "    train_data_adjacent, val_data_adjacent, test_data_adjacent = split(graph_data_adjacent)\n",
    "    # print(train_data_adjacent)\n",
    "    train_graph_data, val_graph_data, test_graph_data = split_2(graph_data)\n",
    "    # print(train_graph_data)\n",
    "\n",
    "    # Combine internal edges with training data adjacent edges\n",
    "    train_data_combined = combine_edges(train_graph_data, train_data_adjacent)\n",
    "    train_data.append(train_data_combined)\n",
    "\n",
    "\n",
    "\n",
    "    # Combine internal edges with validation data adjacent edges\n",
    "    val_data_combined = combine_edges(val_graph_data, val_data_adjacent)\n",
    "    val_data.append(val_data_combined)\n",
    "    # Combine internal edges with testing data adjacent edges\n",
    "    test_data_combined = combine_edges(test_graph_data, test_data_adjacent)\n",
    "    test_data.append(test_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[144, 300], edge_index=[2, 468], edge_attr=[468], num_nodes=144, edge_label=[30], edge_label_index=[2, 30])\n"
     ]
    }
   ],
   "source": [
    "print(val_data_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.float>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].x.float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[144, 300], edge_index=[2, 468], edge_attr=[468], num_nodes=144, edge_label=[468], edge_label_index=[2, 468], batch=[144], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "\n",
    "def train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100\n",
    "):\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for idx in range(len(train_data)):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            data = train_data[idx]\n",
    "            z = model.encode(data.x.float(), data.edge_index)\n",
    "\n",
    "            # sampling training negatives for every training epoch\n",
    "            neg_edge_index = negative_sampling(\n",
    "                edge_index=data.edge_index, num_nodes=data.num_nodes,\n",
    "                num_neg_samples=data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "            edge_label_index = torch.cat(\n",
    "                [data.edge_label_index, neg_edge_index],\n",
    "                dim=-1,\n",
    "            )\n",
    "            edge_label = torch.cat([\n",
    "                data.edge_label,\n",
    "                data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "            ], dim=0)\n",
    "\n",
    "            out = model.decode(z, edge_label_index).view(-1)\n",
    "            loss = criterion(out, edge_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            val_auc = eval_link_predictor(model, val_data[idx])\n",
    "            \n",
    "        print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "\n",
    "    model.eval()\n",
    "    z = model.encode(data.x.float(), data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.696, Val AUC: 0.520\n",
      "Epoch: 002, Train Loss: 0.695, Val AUC: 0.500\n",
      "Epoch: 003, Train Loss: 0.694, Val AUC: 0.500\n",
      "Epoch: 004, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 005, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 006, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 007, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 008, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 009, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 010, Train Loss: 0.694, Val AUC: 0.500\n",
      "Epoch: 011, Train Loss: 0.693, Val AUC: 0.500\n",
      "Epoch: 012, Train Loss: 0.704, Val AUC: 0.500\n",
      "Epoch: 013, Train Loss: 0.693, Val AUC: 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_link_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m test_auc \u001b[38;5;241m=\u001b[39m eval_link_predictor(model, test_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 32\u001b[0m, in \u001b[0;36mtrain_link_predictor\u001b[0;34m(model, train_data, val_data, optimizer, criterion, n_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode(z, edge_label_index)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, edge_label)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m val_auc \u001b[38;5;241m=\u001b[39m eval_link_predictor(model, val_data[idx])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net(300, 128, 64)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "model = train_link_predictor(model, train_data, val_data, optimizer, criterion)\n",
    "test_auc = eval_link_predictor(model, test_data)\n",
    "print(f\"Test: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
