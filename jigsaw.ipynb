{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        # Add more convolutional layers if needed\n",
    "        self.shuffle = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1)\n",
    "        # Assume the size of residual blocks output is 256xHf'xWf'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        # Shuffle operation\n",
    "        x = self.shuffle(x)\n",
    "        return x\n",
    "\n",
    "class ClassificationModule(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationModule, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.LeakyReLU()(x)  # Using LeakyReLU for the final layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')  # Upsampling layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.Tanh()(x)  # Using Tanh for the final layer to output values in the range [-1, 1]\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=4, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.sigmoid(x)  # Using Sigmoid activation for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        # Get the output size of the encoder\n",
    "        self.encoder.eval()  # Set the encoder to evaluation mode to get the output size\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.randn(1, 3, 256, 256)  # Assuming input size of 256x256\n",
    "            encoded_features = self.encoder(sample_input)\n",
    "            input_size = encoded_features.view(encoded_features.size(0), -1).size(1)  # Flattened size\n",
    "\n",
    "        self.classification_module = ClassificationModule(input_size, num_classes=10)  # Assuming 10 classes for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the entire network\n",
    "        encoded_features = self.encoder(x)\n",
    "        classification_output = self.classification_module(encoded_features)\n",
    "        generated_output = self.generator(encoded_features)\n",
    "        discriminator_output = self.discriminator(generated_output)\n",
    "        return classification_output, generated_output, discriminator_output\n",
    "\n",
    "# Define loss functions\n",
    "classification_criterion = nn.CrossEntropyLoss()\n",
    "generator_criterion = nn.MSELoss()\n",
    "discriminator_criterion = nn.BCELoss()\n",
    "\n",
    "# Initialize the network\n",
    "network = Network()\n",
    "\n",
    "# Define optimizers for each component\n",
    "optimizer_classification = optim.Adam(list(network.encoder.parameters()) + \n",
    "                                       list(network.classification_module.parameters()), lr=0.001)\n",
    "optimizer_generator = optim.Adam(network.generator.parameters(), lr=0.001)\n",
    "optimizer_discriminator = optim.Adam(network.discriminator.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train_loader, num_epochs, real_labels, fake_labels, target_data, \n",
    "                  classification_criterion, generator_criterion, discriminator_criterion, \n",
    "                  optimizer_classification, optimizer_generator, optimizer_discriminator, log_interval=100):\n",
    "    classification_losses = []\n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (input_data, target_labels) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            classification_output, generated_output, discriminator_output = network(input_data)\n",
    "\n",
    "            # Compute classification loss\n",
    "            classification_loss = classification_criterion(classification_output, target_labels)\n",
    "\n",
    "            # Compute generator loss\n",
    "            generator_loss = generator_criterion(generated_output, target_data)\n",
    "\n",
    "            # Compute discriminator loss\n",
    "            real_loss = discriminator_criterion(discriminator_output, real_labels)\n",
    "            fake_output = network.discriminator(target_data.detach())  # Detach to prevent backpropagation through generator\n",
    "            fake_loss = discriminator_criterion(fake_output, fake_labels)\n",
    "            discriminator_loss = real_loss + fake_loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_classification.zero_grad()\n",
    "            optimizer_generator.zero_grad()\n",
    "            optimizer_discriminator.zero_grad()\n",
    "\n",
    "            classification_loss.backward()\n",
    "            generator_loss.backward()\n",
    "            discriminator_loss.backward()\n",
    "\n",
    "            optimizer_classification.step()\n",
    "            optimizer_generator.step()\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # Append losses to lists\n",
    "            classification_losses.append(classification_loss.item())\n",
    "            generator_losses.append(generator_loss.item())\n",
    "            discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "            # Print loss statistics\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Epoch [{}/{}], Batch [{}/{}], '\n",
    "                      'Classification Loss: {:.4f}, '\n",
    "                      'Generator Loss: {:.4f}, '\n",
    "                      'Discriminator Loss: {:.4f}'.format(\n",
    "                          epoch+1, num_epochs, batch_idx+1, len(train_loader),\n",
    "                          classification_loss.item(),\n",
    "                          generator_loss.item(),\n",
    "                          discriminator_loss.item()))\n",
    "                \n",
    "                # Print additional feedback\n",
    "                print('Avg Classification Loss: {:.4f}, '\n",
    "                      'Avg Generator Loss: {:.4f}, '\n",
    "                      'Avg Discriminator Loss: {:.4f}'.format(\n",
    "                          sum(classification_losses) / len(classification_losses),\n",
    "                          sum(generator_losses) / len(generator_losses),\n",
    "                          sum(discriminator_losses) / len(discriminator_losses)))\n",
    "\n",
    "    return classification_losses, generator_losses, discriminator_losses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
