{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_image(image):\n",
    "    parts = []\n",
    "    height, width, _ = image.shape\n",
    "    part_height = height // 3\n",
    "    part_width = width // 3\n",
    "    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            part = image[i*part_height:(i+1)*part_height, j*part_width:(j+1)*part_width]\n",
    "            parts.append(part)\n",
    "    \n",
    "    return parts\n",
    "\n",
    "def generate_combinations(parts, num_combinations):\n",
    "    combinations = []\n",
    "    original_positions = []\n",
    "    indices = list(range(len(parts)))\n",
    "    \n",
    "    for _ in range(num_combinations):\n",
    "        random.shuffle(indices)\n",
    "        combination = [parts[i] for i in indices]\n",
    "        combinations.append(combination)\n",
    "        original_positions.append(indices.copy())\n",
    "    \n",
    "    return combinations, original_positions\n",
    "def stitch_shuffled_image(parts):\n",
    "    num_parts = len(parts)\n",
    "    part_size = parts[0].shape[0]  # Assuming all parts are square\n",
    "    \n",
    "    stitched_image_size = int(np.sqrt(num_parts) * part_size)\n",
    "    stitched_image = np.zeros((stitched_image_size, stitched_image_size, parts[0].shape[2]), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(stitched_image.shape[0] // part_size):\n",
    "        for j in range(stitched_image.shape[1] // part_size):\n",
    "            part_index = i * int(stitched_image.shape[0] / part_size) + j\n",
    "            stitched_image[i*part_size:(i+1)*part_size, j*part_size:(j+1)*part_size] = parts[part_index]\n",
    "    \n",
    "    return stitched_image\n",
    "# checking if the target sequence is correct\n",
    "def reconstructed_image(img, non_converted_target_data, test=False):\n",
    "    \n",
    "    sequenced = [0] * 9\n",
    "    for i in range(0,27,3):\n",
    "\n",
    "        # stack the 3 channels to get the original image\n",
    "        blue_channel = img[:,:,i]\n",
    "        green_channel = img[:,:,i+1]\n",
    "        red_channel = img[:,:,i+2]\n",
    "\n",
    "        # stack the 3 channels to get the original image\n",
    "        tile = np.stack((blue_channel, green_channel, red_channel), axis=2)\n",
    "\n",
    "        if test:\n",
    "            sequenced[i // 3] = tile\n",
    "        else:\n",
    "            sequenced[non_converted_target_data[i // 3]] = tile\n",
    "\n",
    "    stiched_img = stitch_shuffled_image(sequenced)\n",
    "    return stiched_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawDataset(Dataset):\n",
    "    def __init__(self, input_data, target_data, transform=None):\n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.input_data[idx]\n",
    "        target = self.target_data[idx]\n",
    "        \n",
    "        # Convert NumPy arrays to float tensors\n",
    "        image = torch.from_numpy(image).float()\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the JigsawModel class with batch normalization layers\n",
    "class JigsawModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JigsawModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(27, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization layer\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)  # Batch normalization layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 10 * 10, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512) \n",
    "        self.fc4 = nn.Linear(512, 81)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 128 * 10 * 10)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoundaryLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        batch_size = outputs.size(0)\n",
    "        top_bottom_ssim = torch.zeros(batch_size)\n",
    "        left_right_ssim = torch.zeros(batch_size)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # Reshape the outputs and labels to match SSIM function requirements\n",
    "            output_img = outputs[i].view(9, 9).unsqueeze(0).unsqueeze(0)\n",
    "            label_img = labels[i].view(9, 9).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Compute SSIM\n",
    "            top_bottom_ssim[i] = 1 - torch.mean(torch.abs(output_img[0, :, 0, :] - label_img[0, :, -1, :]))\n",
    "            left_right_ssim[i] = 1 - torch.mean(torch.abs(output_img[0, 0, :, :] - label_img[0, -1, :, :]))\n",
    "\n",
    "        avg_tb_ssim = torch.mean(top_bottom_ssim)\n",
    "        avg_lr_ssim = torch.mean(left_right_ssim)\n",
    "\n",
    "        loss = avg_tb_ssim + avg_lr_ssim\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "image_dir = \"cavallo\"\n",
    "\n",
    "images = []\n",
    "for filename in os.listdir(image_dir):\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is not None:\n",
    "        image = cv2.resize(image, (120, 120))\n",
    "        images.append(image)\n",
    "input_data = []\n",
    "target_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    parts = divide_image(image)\n",
    "    combinations, original_positions = generate_combinations(parts, 10)\n",
    "    \n",
    "    for idx, combination in enumerate(combinations):\n",
    "\n",
    "        # shape of combination is (9, 40, 40, 3)\n",
    "        combination = np.array(combination).transpose(0, 3, 1, 2)\n",
    "        combination = np.concatenate(combination, axis=0).transpose(1, 2, 0)\n",
    "        input_data.append(combination)\n",
    "\n",
    "        dummy_target = np.zeros((9, 9), dtype=np.uint8)\n",
    "        for i in range(9):\n",
    "            dummy_target[i, original_positions[idx][i]] = 1\n",
    "\n",
    "        target_data.append(dummy_target.flatten())\n",
    "\n",
    "input_data = np.array(input_data)\n",
    "target_data = np.array(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JigsawModel()\n",
    "boundary_loss_fn = BoundaryLoss()\n",
    "adversarial_loss_fn = nn.BCELoss()\n",
    "weight_adversarial = 0.5  # Adjust as needed\n",
    "weight_boundary = 0.5 \n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data: (20984, 40, 40, 27)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
    "print(\"shape of training data:\", X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Apply normalization transform to your dataset\n",
    "train_dataset = JigsawDataset(X_train, y_train)\n",
    "test_dataset = JigsawDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch:   100, Loss: 0.714\n",
      "Epoch: 1, Batch:   200, Loss: 0.708\n",
      "Epoch: 1, Batch:   300, Loss: 0.706\n",
      "Epoch [1/10], Loss: 19.01365751028061, Accuracy: 0.0\n",
      "Epoch: 2, Batch:   100, Loss: 0.703\n",
      "Epoch: 2, Batch:   200, Loss: 0.700\n",
      "Epoch: 2, Batch:   300, Loss: 0.699\n",
      "Epoch [2/10], Loss: 18.83733904361725, Accuracy: 0.0\n",
      "Epoch: 3, Batch:   100, Loss: 0.695\n",
      "Epoch: 3, Batch:   200, Loss: 0.695\n",
      "Epoch: 3, Batch:   300, Loss: 0.694\n",
      "Epoch [3/10], Loss: 18.732250094413757, Accuracy: 0.0\n",
      "Epoch: 4, Batch:   100, Loss: 0.691\n",
      "Epoch: 4, Batch:   200, Loss: 0.690\n",
      "Epoch: 4, Batch:   300, Loss: 0.690\n",
      "Epoch [4/10], Loss: 18.629630029201508, Accuracy: 0.0\n",
      "Epoch: 5, Batch:   100, Loss: 0.686\n",
      "Epoch: 5, Batch:   200, Loss: 0.686\n",
      "Epoch: 5, Batch:   300, Loss: 0.685\n",
      "Epoch [5/10], Loss: 18.53717303276062, Accuracy: 0.0\n",
      "Epoch: 6, Batch:   100, Loss: 0.682\n",
      "Epoch: 6, Batch:   200, Loss: 0.683\n",
      "Epoch: 6, Batch:   300, Loss: 0.681\n",
      "Epoch [6/10], Loss: 18.378956079483032, Accuracy: 0.0\n",
      "Epoch: 7, Batch:   100, Loss: 0.676\n",
      "Epoch: 7, Batch:   200, Loss: 0.677\n",
      "Epoch: 7, Batch:   300, Loss: 0.677\n",
      "Epoch [7/10], Loss: 18.260789573192596, Accuracy: 0.0\n",
      "Epoch: 8, Batch:   100, Loss: 0.672\n",
      "Epoch: 8, Batch:   200, Loss: 0.671\n",
      "Epoch: 8, Batch:   300, Loss: 0.672\n",
      "Epoch [8/10], Loss: 18.10948222875595, Accuracy: 0.0\n",
      "Epoch: 9, Batch:   100, Loss: 0.666\n",
      "Epoch: 9, Batch:   200, Loss: 0.667\n",
      "Epoch: 9, Batch:   300, Loss: 0.667\n",
      "Epoch [9/10], Loss: 17.993799686431885, Accuracy: 0.0\n",
      "Epoch: 10, Batch:   100, Loss: 0.661\n",
      "Epoch: 10, Batch:   200, Loss: 0.662\n",
      "Epoch: 10, Batch:   300, Loss: 0.662\n",
      "Epoch [10/10], Loss: 17.915777564048767, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Calculate adversarial loss\n",
    "        adversarial_loss = adversarial_loss_fn(outputs, labels.float())\n",
    "        b_loss = boundary_loss_fn(outputs, labels)\n",
    "        total_loss = weight_adversarial * adversarial_loss + weight_boundary * b_loss\n",
    "        # Apply gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print('Epoch: %d, Batch: %5d, Loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted_indices = torch.max(outputs, 1)  # Get the index with the highest probability\n",
    "        predicted_labels = torch.zeros_like(labels)\n",
    "        for i, idx in enumerate(predicted_indices):\n",
    "            predicted_labels[i, idx] = 1  # Convert index to one-hot encoding\n",
    "\n",
    "        correct = (predicted_labels == labels).all(dim=1).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur(sequence, outputs):\n",
    "    sequence = np.array(sequence)\n",
    "    for i in range(len(sequence)):\n",
    "        while np.sum(sequence == sequence[i]) > 1:  # If the current element is a duplicate\n",
    "            # Find the index in outputs for the current sequence element that is not yet in the updated sequence\n",
    "            scores = outputs[i]\n",
    "            sorted_indices = np.argsort(scores)[::-1]  # Indices of scores sorted in descending order\n",
    "            for idx in sorted_indices:\n",
    "                if idx not in sequence:\n",
    "                    sequence[i] = idx\n",
    "                    break\n",
    "                \n",
    "    return sequence.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test images: 0 %\n",
      "Per tile accuracy on test images: 18 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "per_tile_accuracy = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.permute(0, 3, 1, 2).float()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # reshape the output to 9x9 matrix\n",
    "        outputs = outputs.reshape(-1, 9, 9)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # now doing the same for the target\n",
    "        labels = labels.reshape(-1, 9, 9)\n",
    "        target = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # check if the predicted sequence is correct\n",
    "        for i in range(len(predicted)):\n",
    "            updated_predicted = recur(predicted[i], outputs[i, : , :].numpy())\n",
    "            \n",
    "            if torch.equal(torch.tensor(updated_predicted), target[i]):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            per_tile_accuracy += (np.array(updated_predicted) == target[i].numpy()).sum() / 9\n",
    "\n",
    "print('Accuracy on test images: %d %%' % (100 * correct / total))\n",
    "print('Per tile accuracy on test images: %d %%' % (100 * per_tile_accuracy / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'saved_models/jigsaw_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0118e+00,  2.0453e+00,  2.2418e+00,  2.2821e+00,  2.5797e+00,\n",
      "          2.6337e+00,  2.1445e+00,  2.1047e+00,  2.1770e+00],\n",
      "        [-1.5043e+00, -2.8925e-01, -1.1173e+00, -1.2400e+00, -1.4567e+00,\n",
      "         -1.2544e+00, -1.8207e+00, -2.1900e+00, -1.3938e+00],\n",
      "        [-1.7818e+00, -1.1564e+00, -1.0048e+00, -6.2732e-01, -9.2533e-01,\n",
      "         -1.6911e+00, -1.3251e+00, -1.5247e+00, -1.4480e+00],\n",
      "        [-3.1206e+00, -2.0357e+00, -1.3962e+00, -2.1916e+00, -2.2762e+00,\n",
      "         -4.7843e-01, -5.4281e-01, -3.9811e-03,  1.3640e-01],\n",
      "        [-1.8509e+00, -1.0560e+00, -5.7689e-01, -4.8483e-01, -9.5806e-01,\n",
      "         -1.4551e+00, -1.7789e+00, -1.6852e+00, -2.0565e+00],\n",
      "        [-2.6980e+00, -2.1436e+00, -2.1027e+00, -7.1422e-01, -1.8187e+00,\n",
      "         -9.5641e-01, -3.7571e-01, -4.4514e-03, -9.5942e-01],\n",
      "        [-2.3109e+00, -1.0724e+00, -8.4616e-01, -1.3514e+00, -5.1938e-01,\n",
      "         -8.6498e-01, -1.7045e+00, -1.5688e+00, -1.7525e+00],\n",
      "        [-2.3116e+00, -1.4784e+00, -1.3319e+00, -1.0744e+00, -2.2189e+00,\n",
      "         -1.1460e+00, -3.2759e-01, -7.6561e-01, -6.2098e-01],\n",
      "        [ 1.1154e+00, -1.4048e+00, -1.4979e+00, -2.3451e+00, -2.8437e+00,\n",
      "         -3.0648e+00, -2.6843e+00, -3.0387e+00, -3.1788e+00]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "[0 2 3 8 5 7 4 6 1]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(X_test))\n",
    "image = X_test[idx]\n",
    "target = y_test[idx]\n",
    "inputs = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "outputs = model(inputs)\n",
    "outputs = outputs.reshape(9, 9)\n",
    "print(outputs)\n",
    "converted_target = np.argmax(target.reshape(9, 9), axis=1)\n",
    "print(converted_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
