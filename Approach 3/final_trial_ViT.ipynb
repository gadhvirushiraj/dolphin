{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQA_ZtEnSRFJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKzKMaTqU6K",
        "outputId": "dde1969a-3bc8-4932-9d2f-b8883611e5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the directory where your images are stored\n",
        "image_dir = '/content/drive/My Drive/cavallo'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "class ImageTilesDataset(Dataset):\n",
        "    def __init__(self, directory, grid_size=(3, 3), tile_size=100, max_images=100):\n",
        "        self.directory = directory\n",
        "        self.grid_size = grid_size\n",
        "        self.tile_size = tile_size\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.max_images = max_images\n",
        "        self.load_images()\n",
        "\n",
        "    def load_images(self):\n",
        "        file_count = 0\n",
        "        for filename in os.listdir(self.directory):\n",
        "            if file_count >= self.max_images:  # Stop if the maximum number of images is reached\n",
        "                break\n",
        "            if filename.lower().endswith(('.png', '.jpeg', '.jpg')):\n",
        "                file_count += 1\n",
        "                image_path = os.path.join(self.directory, filename)\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "                self.jumble_image(image)\n",
        "\n",
        "        if file_count == 0:\n",
        "            print(\"Error: No images found.\")\n",
        "        else:\n",
        "            print(f\"Loaded {file_count} images.\")\n",
        "\n",
        "    def jumble_image(self, image):\n",
        "        image = Resize((self.grid_size[0] * self.tile_size, self.grid_size[1] * self.tile_size))(image)\n",
        "        tiles = [image.crop((j * self.tile_size, i * self.tile_size, (j + 1) * self.tile_size, (i + 1) * self.tile_size))\n",
        "                 for i in range(self.grid_size[0]) for j in range(self.grid_size[1])]\n",
        "        indices = list(range(len(tiles)))\n",
        "        random.shuffle(indices)\n",
        "        tiles = [tiles[i] for i in indices]\n",
        "        self.data.extend(tiles)\n",
        "        self.labels.extend(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        transform = Compose([\n",
        "            Resize(224),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        tile = transform(self.data[idx])\n",
        "        label = self.labels[idx]\n",
        "        return tile, label\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=224):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = x + self.position_embeddings\n",
        "        return x\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size):\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    return pos_embed\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega\n",
        "\n",
        "    pos = grid.reshape(2, -1)  # (2, HW)\n",
        "    out = np.einsum('d,hw->dhw', omega, pos)  # (D, H, W)\n",
        "\n",
        "    emb_sin = np.sin(out)\n",
        "    emb_cos = np.cos(out)\n",
        "\n",
        "    emb = np.stack([emb_sin, emb_cos], dim=-1)\n",
        "    emb = np.reshape(emb, [1, -1, embed_dim])\n",
        "    return emb\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8, forward_expansion=4, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_size, forward_expansion * emb_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * emb_size, emb_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        attention = self.attention(x, x, x)\n",
        "        x = attention + x\n",
        "        x = self.norm2(x)\n",
        "        forward = self.feed_forward(x)\n",
        "        out = forward + x\n",
        "        return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, patch_size=16, emb_size=768, depth=6, num_heads=8, num_classes=9, img_size=224, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, emb_size=emb_size, img_size=img_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.transformer = nn.Sequential(*[TransformerEncoderLayer(emb_size, num_heads, dropout_rate=dropout_rate) for _ in range(depth)])\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.fc = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = self.transformer(x)\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.fc(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        return self.attention(query, key, value)[0]\n",
        "\n",
        "image_dir = '/content/drive/My Drive/cavallo'\n",
        "dataset = ImageTilesDataset(image_dir, max_images=100)\n",
        "loader = DataLoader(dataset, batch_size=9, shuffle=True)\n",
        "\n",
        "model = VisionTransformer(img_size=224, patch_size=32, num_classes=9, depth=6, num_heads=8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for tiles, labels in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(tiles)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}: Loss: {loss.item()}\")\n",
        "\n",
        "train_model(model, loader, criterion, optimizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htl9GEz_TOcV",
        "outputId": "cfabcc50-4385-46bb-d69f-5f7f9f265d0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 images.\n",
            "Epoch 1: Loss: 2.5916965007781982\n",
            "Epoch 2: Loss: 2.4304134845733643\n",
            "Epoch 3: Loss: 2.4311647415161133\n",
            "Epoch 4: Loss: 2.428215503692627\n",
            "Epoch 5: Loss: 2.5313122272491455\n",
            "Epoch 6: Loss: 2.362938404083252\n",
            "Epoch 7: Loss: 2.39508056640625\n",
            "Epoch 8: Loss: 2.241774797439575\n",
            "Epoch 9: Loss: 2.202528476715088\n",
            "Epoch 10: Loss: 2.140164613723755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def puzzle_accuracy(model, dataset):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            tiles, labels = dataset[i]\n",
        "            tiles = tiles.unsqueeze(0)\n",
        "            labels = torch.tensor(labels).unsqueeze(0)\n",
        "            outputs = model(tiles)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).item()\n",
        "            total += 1\n",
        "    return correct / total\n",
        "\n",
        "puzzle_acc = puzzle_accuracy(model, dataset)\n",
        "print(f\"Puzzle Accuracy: {puzzle_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "r-CD4MUpNnil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc041fc-6e9c-4e1a-97e6-b13a909da313"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puzzle Accuracy: 0.1111\n"
          ]
        }
      ]
    }
  ]
}