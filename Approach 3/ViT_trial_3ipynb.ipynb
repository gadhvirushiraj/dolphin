{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKzKMaTqU6K",
        "outputId": "605c0981-7216-45bb-c8a7-41083a2597ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the directory where your images are stored\n",
        "image_dir = '/content/drive/My Drive/cavallo'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, Resize, RandomCrop, RandomHorizontalFlip, ToTensor, Normalize\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "class ImageTilesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, directory, grid_size=(3, 3), tile_size=100):\n",
        "        self.directory = directory\n",
        "        self.grid_size = grid_size\n",
        "        self.tile_size = tile_size\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.load_images()\n",
        "\n",
        "    def load_images(self):\n",
        "        file_count = 0\n",
        "        for filename in os.listdir(self.directory):\n",
        "            if filename.lower().endswith(('.png', '.jpeg', '.jpg')):\n",
        "                file_count += 1\n",
        "                image_path = os.path.join(self.directory, filename)\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "                self.jumble_image(image)\n",
        "        if file_count == 0:\n",
        "            print(\"No images loaded.\")\n",
        "        else:\n",
        "            print(f\"Loaded {file_count} images.\")\n",
        "\n",
        "    def jumble_image(self, image):\n",
        "        image = Resize((self.grid_size[0] * self.tile_size, self.grid_size[1] * self.tile_size))(image)\n",
        "        tiles = [image.crop((j * self.tile_size, i * self.tile_size, (j + 1) * self.tile_size, (i + 1) * self.tile_size))\n",
        "                 for i in range(self.grid_size[0]) for j in range(self.grid_size[1])]\n",
        "        indices = list(range(len(tiles)))\n",
        "        random.shuffle(indices)\n",
        "        tiles = [tiles[i] for i in indices]\n",
        "        self.data.extend(tiles)\n",
        "        self.labels.extend(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        transform = Compose([\n",
        "            Resize(256),  # Slightly larger resize\n",
        "            RandomCrop(224),  # Random crop to the final size\n",
        "            RandomHorizontalFlip(),  # Horizontal flip\n",
        "            ToTensor(),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        tile = transform(self.data[idx])\n",
        "        label = self.labels[idx]\n",
        "        return tile, label\n",
        "\n",
        "# Include your updated VisionTransformer and its component classes here.\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=100):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)  # (B, C, H/P, W/P)\n",
        "        x = x.flatten(2)         # (B, C, N) where N is number of patches\n",
        "        x = x.transpose(1, 2)    # (B, N, C)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        return self.attention(query, key, value)[0]\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8, forward_expansion=4, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout_rate)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_size, forward_expansion * emb_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * emb_size, emb_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x)\n",
        "        attention = self.attention(x, x, x)\n",
        "        x = attention + x\n",
        "        x = self.norm2(x)\n",
        "        forward = self.feed_forward(x)\n",
        "        out = forward + x\n",
        "        return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, patch_size=16, emb_size=768, depth=6, num_heads=8, num_classes=9, img_size=100, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, emb_size=emb_size, img_size=img_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros((img_size // patch_size) ** 2 + 1, emb_size))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.transformer = nn.Sequential(*[TransformerEncoderLayer(emb_size, num_heads, dropout_rate=dropout_rate) for _ in range(depth)])\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.fc = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.position_embeddings[:n+1]\n",
        "        x = self.transformer(x)\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.fc(x)\n",
        "\n",
        "# Set up training components\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image_dir = '/content/drive/My Drive/cavallo'\n",
        "dataset = ImageTilesDataset(image_dir)\n",
        "loader = DataLoader(dataset, batch_size=9, shuffle=True, num_workers=4)\n",
        "model = VisionTransformer(img_size=224, patch_size=32, num_classes=9, depth=6, num_heads=8).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "def train_model(model, loader, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for tiles, labels in loader:\n",
        "            tiles, labels = tiles.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(tiles)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        scheduler.step()  # Adjust the learning rate\n",
        "        average_loss = total_loss / len(loader)\n",
        "        print(f\"Epoch {epoch+1}: Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "train_model(model, loader, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w3IGAsVBnii",
        "outputId": "8761eb91-d34c-4d5f-b549-4519ed46d932"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2623 images.\n",
            "Epoch 1: Average Loss: 2.2626\n",
            "Epoch 2: Average Loss: 2.2109\n",
            "Epoch 3: Average Loss: 2.2063\n",
            "Epoch 4: Average Loss: 2.1981\n",
            "Epoch 5: Average Loss: 2.1979\n",
            "Epoch 6: Average Loss: 2.1977\n",
            "Epoch 7: Average Loss: 2.1974\n",
            "Epoch 8: Average Loss: 2.1973\n",
            "Epoch 9: Average Loss: 2.1974\n",
            "Epoch 10: Average Loss: 2.1973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def piecewise_accuracy(model, loader):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for tiles, labels in loader:\n",
        "#             tiles, labels = tiles.to(device), labels.to(device)\n",
        "#             outputs = model(tiles)\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "#             total += labels.size(0)\n",
        "#     return correct / total\n",
        "\n",
        "def puzzle_accuracy(model, dataset):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(dataset)):\n",
        "            tiles, labels = dataset[i]\n",
        "            tiles = tiles.unsqueeze(0).to(device)  # Add batch dimension\n",
        "            labels = torch.tensor(labels).unsqueeze(0).to(device)  # Add batch dimension\n",
        "            outputs = model(tiles)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).item()\n",
        "            total += 1\n",
        "    return correct / total\n",
        "\n",
        "# # Calculate piece-wise accuracy and puzzle accuracy\n",
        "# piecewise_acc = piecewise_accuracy(model, loader)\n",
        "# print(f\"Piece-wise Accuracy: {piecewise_acc:.4f}\")\n",
        "\n",
        "puzzle_acc = puzzle_accuracy(model, dataset)\n",
        "print(f\"Puzzle Accuracy: {puzzle_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpz6QwvGDNtP",
        "outputId": "34089424-bc6f-4563-db22-7cd170a1c5c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Piece-wise Accuracy: 0.1111\n",
            "Puzzle Accuracy: 0.1111\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}