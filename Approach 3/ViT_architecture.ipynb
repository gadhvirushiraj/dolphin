{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Y5S7p9hD16",
        "outputId": "e598f4a4-48bc-4273-d215-1af2a640b0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embedding): PatchEmbedding(\n",
            "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (transformer_blocks): ModuleList(\n",
            "    (0-11): 12 x TransformerBlock(\n",
            "      (attention): MultiHeadAttention(\n",
            "        (keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (queries): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (values): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (fc_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (feed_forward): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (to_cls_token): Identity()\n",
            "  (mlp_head): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=16, emb_size=768, img_size=100):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)  # shape (B, C, H/P, W/P)\n",
        "        x = x.flatten(2)  # flatten the last two dimensions\n",
        "        x = x.transpose(1, 2)  # shape (B, num_patches, emb_size)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        assert self.head_dim * num_heads == emb_size, \"emb_size must be divisible by num_heads\"\n",
        "\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.fc_out = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        keys = self.keys(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        queries = self.queries(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "        values = self.values(x).reshape(B, N, self.num_heads, self.head_dim)\n",
        "\n",
        "        energy = torch.einsum(\"bnqd,bnkd->bnqk\", [queries, keys]) / math.sqrt(self.head_dim)\n",
        "        attention = torch.softmax(energy, dim=-1)  # shape (B, num_heads, N, N)\n",
        "\n",
        "        out = torch.einsum(\"bnqk,bnkd->bnqd\", [attention, values]).reshape(B, N, self.emb_size)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_size=768, num_heads=8, forward_expansion=4):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(emb_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(emb_size, forward_expansion * emb_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * emb_size, emb_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention = self.attention(x)\n",
        "        x = self.norm1(attention + x)\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(forward + x)\n",
        "        return out\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=100, patch_size=16, num_patches=36, emb_size=768, depth=12, num_heads=8, num_classes=9):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, emb_size=emb_size, img_size=img_size)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, 1 + num_patches, emb_size))\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(emb_size, num_heads)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        cls_token = self.cls_token.repeat(x.shape[0], 1, 1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        x += self.position_embeddings\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x)\n",
        "\n",
        "        cls_token_final = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(cls_token_final)\n",
        "\n",
        "# Model instantiation\n",
        "model = VisionTransformer()\n",
        "print(model)\n"
      ]
    }
  ]
}