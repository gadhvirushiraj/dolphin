{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFlIdK_VM1rc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torchvision.transforms import Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rUuMcEAGM1rg"
      },
      "outputs": [],
      "source": [
        "def divide_image(image):\n",
        "    parts = []\n",
        "    height, width, _ = image.shape\n",
        "    part_height = height // 3\n",
        "    part_width = width // 3\n",
        "\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            part = image[i*part_height:(i+1)*part_height, j*part_width:(j+1)*part_width]\n",
        "            parts.append(part)\n",
        "\n",
        "    return parts\n",
        "\n",
        "def generate_combinations(parts, num_combinations):\n",
        "    combinations = []\n",
        "    original_positions = []\n",
        "    indices = list(range(len(parts)))\n",
        "\n",
        "    for _ in range(num_combinations):\n",
        "        random.shuffle(indices)\n",
        "        combination = [parts[i] for i in indices]\n",
        "        combinations.append(combination)\n",
        "        original_positions.append(indices.copy())\n",
        "\n",
        "    return combinations, original_positions\n",
        "def stitch_shuffled_image(parts):\n",
        "    num_parts = len(parts)\n",
        "    part_size = parts[0].shape[0]  # Assuming all parts are square\n",
        "\n",
        "    stitched_image_size = int(np.sqrt(num_parts) * part_size)\n",
        "    stitched_image = np.zeros((stitched_image_size, stitched_image_size, parts[0].shape[2]), dtype=np.uint8)\n",
        "\n",
        "    for i in range(stitched_image.shape[0] // part_size):\n",
        "        for j in range(stitched_image.shape[1] // part_size):\n",
        "            part_index = i * int(stitched_image.shape[0] / part_size) + j\n",
        "            stitched_image[i*part_size:(i+1)*part_size, j*part_size:(j+1)*part_size] = parts[part_index]\n",
        "\n",
        "    return stitched_image\n",
        "# checking if the target sequence is correct\n",
        "def reconstructed_image(img, non_converted_target_data, test=False):\n",
        "\n",
        "    sequenced = [0] * 9\n",
        "    for i in range(0,27,3):\n",
        "\n",
        "        # stack the 3 channels to get the original image\n",
        "        blue_channel = img[:,:,i]\n",
        "        green_channel = img[:,:,i+1]\n",
        "        red_channel = img[:,:,i+2]\n",
        "\n",
        "        # stack the 3 channels to get the original image\n",
        "        tile = np.stack((blue_channel, green_channel, red_channel), axis=2)\n",
        "\n",
        "        if test:\n",
        "            sequenced[i // 3] = tile\n",
        "        else:\n",
        "            sequenced[non_converted_target_data[i // 3]] = tile\n",
        "\n",
        "    stiched_img = stitch_shuffled_image(sequenced)\n",
        "    return stiched_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "it3ag900M1ri"
      },
      "outputs": [],
      "source": [
        "class JigsawDataset(Dataset):\n",
        "    def __init__(self, input_data, target_data, transform=None):\n",
        "        self.input_data = input_data\n",
        "        self.target_data = target_data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.input_data[idx]\n",
        "        target = self.target_data[idx]\n",
        "\n",
        "        # Convert NumPy arrays to float tensors\n",
        "        image = torch.from_numpy(image).float()\n",
        "        target = torch.from_numpy(target)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g2IviKZnM1ri"
      },
      "outputs": [],
      "source": [
        "# Define the JigsawModel class with batch normalization layers\n",
        "class JigsawModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(JigsawModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(27, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization layer\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)  # Batch normalization layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 10 * 10, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 512)\n",
        "        self.fc4 = nn.Linear(512, 81)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.reshape(-1, 128 * 10 * 10)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EJWQ5aYrM1ri"
      },
      "outputs": [],
      "source": [
        "class BoundaryLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BoundaryLoss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        batch_size = outputs.size(0)\n",
        "        top_bottom_ssim = torch.zeros(batch_size)\n",
        "        left_right_ssim = torch.zeros(batch_size)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Reshape the outputs and labels to match SSIM function requirements\n",
        "            output_img = outputs[i].view(9, 9).unsqueeze(0).unsqueeze(0)\n",
        "            label_img = labels[i].view(9, 9).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # Compute SSIM\n",
        "            top_bottom_ssim[i] = 1 - torch.mean(torch.abs(output_img[0, :, 0, :] - label_img[0, :, -1, :]))\n",
        "            left_right_ssim[i] = 1 - torch.mean(torch.abs(output_img[0, 0, :, :] - label_img[0, -1, :, :]))\n",
        "\n",
        "        avg_tb_ssim = torch.mean(top_bottom_ssim)\n",
        "        avg_lr_ssim = torch.mean(left_right_ssim)\n",
        "\n",
        "        loss = avg_tb_ssim + avg_lr_ssim\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Wh0jJbM1rj",
        "outputId": "68092543-0e20-4c3c-f274-d1de1b24673d"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# image_dir = \"/content/drive/MyDrive/cavallo\"\n",
        "image_dir = \"cavallo\"\n",
        "images = []\n",
        "for filename in os.listdir(image_dir):\n",
        "    image_path = os.path.join(image_dir, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, (120, 120))\n",
        "        images.append(image)\n",
        "input_data = []\n",
        "target_data = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dfAPuKO9M1rj"
      },
      "outputs": [],
      "source": [
        "for image in images:\n",
        "    parts = divide_image(image)\n",
        "    combinations, original_positions = generate_combinations(parts, 10)\n",
        "\n",
        "    for idx, combination in enumerate(combinations):\n",
        "\n",
        "        # shape of combination is (9, 40, 40, 3)\n",
        "        combination = np.array(combination).transpose(0, 3, 1, 2)\n",
        "        combination = np.concatenate(combination, axis=0).transpose(1, 2, 0)\n",
        "        input_data.append(combination)\n",
        "\n",
        "        dummy_target = np.zeros((9, 9), dtype=np.uint8)\n",
        "        for i in range(9):\n",
        "            dummy_target[i, original_positions[idx][i]] = 1\n",
        "\n",
        "        target_data.append(dummy_target.flatten())\n",
        "\n",
        "input_data = np.array(input_data)\n",
        "target_data = np.array(target_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C4QxHZvNM1rk"
      },
      "outputs": [],
      "source": [
        "# model = JigsawModel()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n",
        "model = JigsawModel().to(device)\n",
        "model.load_state_dict(torch.load('bloss_model_38_50.pth'))\n",
        "boundary_loss_fn = BoundaryLoss()\n",
        "adversarial_loss_fn = nn.CrossEntropyLoss()\n",
        "weight_adversarial = 0.5  # Adjust as needed\n",
        "weight_boundary = 0.5\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5jev37WM1rk",
        "outputId": "1449802c-7335-4115-facb-6a7695a04838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of training data: (20984, 40, 40, 27)\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(input_data, target_data, test_size=0.2, random_state=42)\n",
        "print(\"shape of training data:\", X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Apply normalization transform to your dataset\n",
        "train_dataset = JigsawDataset(X_train, y_train)\n",
        "test_dataset = JigsawDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Us8eohnOOfa0",
        "outputId": "ba7c84b6-002f-4253-f7b1-cfb5cc4aec51"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VpzLGUbNM1rl",
        "outputId": "f0058bc8-d762-424e-d56b-a3d946c39911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Batch:   100, Loss: 19.701\n",
            "Epoch: 1, Batch:   200, Loss: 19.261\n",
            "Epoch: 1, Batch:   300, Loss: 18.746\n",
            "Epoch [1/50], Loss: 500.23755645751953, Accuracy: 0.02441704892966361\n",
            "Epoch: 2, Batch:   100, Loss: 18.264\n",
            "Epoch: 2, Batch:   200, Loss: 18.015\n",
            "Epoch: 2, Batch:   300, Loss: 17.767\n",
            "Epoch [2/50], Loss: 479.5336856842041, Accuracy: 0.04544151376146789\n",
            "Epoch: 3, Batch:   100, Loss: 17.452\n",
            "Epoch: 3, Batch:   200, Loss: 17.409\n",
            "Epoch: 3, Batch:   300, Loss: 17.305\n",
            "Epoch [3/50], Loss: 468.5699882507324, Accuracy: 0.05948967889908257\n",
            "Epoch: 4, Batch:   100, Loss: 17.015\n",
            "Epoch: 4, Batch:   200, Loss: 17.023\n",
            "Epoch: 4, Batch:   300, Loss: 16.962\n",
            "Epoch [4/50], Loss: 455.6283531188965, Accuracy: 0.06474579510703364\n",
            "Epoch: 5, Batch:   100, Loss: 16.681\n",
            "Epoch: 5, Batch:   200, Loss: 16.672\n",
            "Epoch: 5, Batch:   300, Loss: 16.617\n",
            "Epoch [5/50], Loss: 449.9295406341553, Accuracy: 0.06880733944954129\n",
            "Epoch: 6, Batch:   100, Loss: 16.347\n",
            "Epoch: 6, Batch:   200, Loss: 16.353\n",
            "Epoch: 6, Batch:   300, Loss: 16.303\n",
            "Epoch [6/50], Loss: 438.57769203186035, Accuracy: 0.07382454128440367\n",
            "Epoch: 7, Batch:   100, Loss: 15.989\n",
            "Epoch: 7, Batch:   200, Loss: 16.033\n",
            "Epoch: 7, Batch:   300, Loss: 16.055\n",
            "Epoch [7/50], Loss: 433.13311100006104, Accuracy: 0.079940749235474\n",
            "Epoch: 8, Batch:   100, Loss: 15.712\n",
            "Epoch: 8, Batch:   200, Loss: 15.733\n",
            "Epoch: 8, Batch:   300, Loss: 15.655\n",
            "Epoch [8/50], Loss: 423.8915681838989, Accuracy: 0.08094418960244648\n",
            "Epoch: 9, Batch:   100, Loss: 15.387\n",
            "Epoch: 9, Batch:   200, Loss: 15.377\n",
            "Epoch: 9, Batch:   300, Loss: 15.394\n",
            "Epoch [9/50], Loss: 413.7619352340698, Accuracy: 0.08438455657492355\n",
            "Epoch: 10, Batch:   100, Loss: 14.994\n",
            "Epoch: 10, Batch:   200, Loss: 15.035\n",
            "Epoch: 10, Batch:   300, Loss: 15.068\n",
            "Epoch [10/50], Loss: 407.698899269104, Accuracy: 0.08992737003058104\n",
            "Epoch: 11, Batch:   100, Loss: 14.630\n",
            "Epoch: 11, Batch:   200, Loss: 14.677\n",
            "Epoch: 11, Batch:   300, Loss: 14.694\n",
            "Epoch [11/50], Loss: 396.2433252334595, Accuracy: 0.095565749235474\n",
            "Epoch: 12, Batch:   100, Loss: 14.269\n",
            "Epoch: 12, Batch:   200, Loss: 14.328\n",
            "Epoch: 12, Batch:   300, Loss: 14.334\n",
            "Epoch [12/50], Loss: 387.8655004501343, Accuracy: 0.09871941896024465\n",
            "Epoch: 13, Batch:   100, Loss: 13.926\n",
            "Epoch: 13, Batch:   200, Loss: 13.946\n",
            "Epoch: 13, Batch:   300, Loss: 13.974\n",
            "Epoch [13/50], Loss: 376.1044406890869, Accuracy: 0.10431001529051988\n",
            "Epoch: 14, Batch:   100, Loss: 13.575\n",
            "Epoch: 14, Batch:   200, Loss: 13.595\n",
            "Epoch: 14, Batch:   300, Loss: 13.573\n",
            "Epoch [14/50], Loss: 366.5925226211548, Accuracy: 0.10980504587155963\n",
            "Epoch: 15, Batch:   100, Loss: 13.190\n",
            "Epoch: 15, Batch:   200, Loss: 13.241\n",
            "Epoch: 15, Batch:   300, Loss: 13.238\n",
            "Epoch [15/50], Loss: 358.4287996292114, Accuracy: 0.10641246177370031\n",
            "Epoch: 16, Batch:   100, Loss: 12.842\n",
            "Epoch: 16, Batch:   200, Loss: 12.896\n",
            "Epoch: 16, Batch:   300, Loss: 12.916\n",
            "Epoch [16/50], Loss: 349.24925231933594, Accuracy: 0.11214640672782875\n",
            "Epoch: 17, Batch:   100, Loss: 12.511\n",
            "Epoch: 17, Batch:   200, Loss: 12.592\n",
            "Epoch: 17, Batch:   300, Loss: 12.584\n",
            "Epoch [17/50], Loss: 339.98519802093506, Accuracy: 0.11305428134556575\n",
            "Epoch: 18, Batch:   100, Loss: 12.246\n",
            "Epoch: 18, Batch:   200, Loss: 12.277\n",
            "Epoch: 18, Batch:   300, Loss: 12.312\n",
            "Epoch [18/50], Loss: 332.9976749420166, Accuracy: 0.11257645259938838\n",
            "Epoch: 19, Batch:   100, Loss: 12.044\n",
            "Epoch: 19, Batch:   200, Loss: 12.048\n",
            "Epoch: 19, Batch:   300, Loss: 12.037\n",
            "Epoch [19/50], Loss: 325.28431701660156, Accuracy: 0.11243310397553517\n",
            "Epoch: 20, Batch:   100, Loss: 11.754\n",
            "Epoch: 20, Batch:   200, Loss: 11.841\n",
            "Epoch: 20, Batch:   300, Loss: 11.852\n",
            "Epoch [20/50], Loss: 321.07371520996094, Accuracy: 0.11233753822629969\n",
            "Epoch: 21, Batch:   100, Loss: 11.603\n",
            "Epoch: 21, Batch:   200, Loss: 11.646\n",
            "Epoch: 21, Batch:   300, Loss: 11.688\n",
            "Epoch [21/50], Loss: 314.97125911712646, Accuracy: 0.11233753822629969\n",
            "Epoch: 22, Batch:   100, Loss: 11.467\n",
            "Epoch: 22, Batch:   200, Loss: 11.492\n",
            "Epoch: 22, Batch:   300, Loss: 11.522\n",
            "Epoch [22/50], Loss: 310.59845638275146, Accuracy: 0.11515672782874618\n",
            "Epoch: 23, Batch:   100, Loss: 11.341\n",
            "Epoch: 23, Batch:   200, Loss: 11.393\n",
            "Epoch: 23, Batch:   300, Loss: 11.379\n",
            "Epoch [23/50], Loss: 307.91026496887207, Accuracy: 0.11185970948012232\n",
            "Epoch: 24, Batch:   100, Loss: 11.251\n",
            "Epoch: 24, Batch:   200, Loss: 11.240\n",
            "Epoch: 24, Batch:   300, Loss: 11.302\n",
            "Epoch [24/50], Loss: 306.26276683807373, Accuracy: 0.11721139143730887\n",
            "Epoch: 25, Batch:   100, Loss: 11.181\n",
            "Epoch: 25, Batch:   200, Loss: 11.148\n",
            "Epoch: 25, Batch:   300, Loss: 11.192\n",
            "Epoch [25/50], Loss: 303.88347911834717, Accuracy: 0.11262423547400612\n",
            "Epoch: 26, Batch:   100, Loss: 11.104\n",
            "Epoch: 26, Batch:   200, Loss: 11.116\n",
            "Epoch: 26, Batch:   300, Loss: 11.087\n",
            "Epoch [26/50], Loss: 299.547815322876, Accuracy: 0.11936162079510704\n",
            "Epoch: 27, Batch:   100, Loss: 11.025\n",
            "Epoch: 27, Batch:   200, Loss: 11.042\n",
            "Epoch: 27, Batch:   300, Loss: 11.066\n",
            "Epoch [27/50], Loss: 297.84939098358154, Accuracy: 0.11467889908256881\n",
            "Epoch: 28, Batch:   100, Loss: 10.951\n",
            "Epoch: 28, Batch:   200, Loss: 10.950\n",
            "Epoch: 28, Batch:   300, Loss: 10.999\n",
            "Epoch [28/50], Loss: 297.07616424560547, Accuracy: 0.11228975535168195\n",
            "Epoch: 29, Batch:   100, Loss: 10.931\n",
            "Epoch: 29, Batch:   200, Loss: 10.953\n",
            "Epoch: 29, Batch:   300, Loss: 10.926\n",
            "Epoch [29/50], Loss: 294.5088806152344, Accuracy: 0.1162079510703364\n",
            "Epoch: 30, Batch:   100, Loss: 10.891\n",
            "Epoch: 30, Batch:   200, Loss: 10.884\n",
            "Epoch: 30, Batch:   300, Loss: 10.930\n",
            "Epoch [30/50], Loss: 294.9056587219238, Accuracy: 0.11501337920489296\n",
            "Epoch: 31, Batch:   100, Loss: 10.868\n",
            "Epoch: 31, Batch:   200, Loss: 10.855\n",
            "Epoch: 31, Batch:   300, Loss: 10.903\n",
            "Epoch [31/50], Loss: 294.1700801849365, Accuracy: 0.11123853211009174\n",
            "Epoch: 32, Batch:   100, Loss: 10.796\n",
            "Epoch: 32, Batch:   200, Loss: 10.842\n",
            "Epoch: 32, Batch:   300, Loss: 10.836\n",
            "Epoch [32/50], Loss: 293.4520568847656, Accuracy: 0.1168769113149847\n",
            "Epoch: 33, Batch:   100, Loss: 10.782\n",
            "Epoch: 33, Batch:   200, Loss: 10.803\n",
            "Epoch: 33, Batch:   300, Loss: 10.787\n",
            "Epoch [33/50], Loss: 292.2902784347534, Accuracy: 0.11530007645259939\n",
            "Epoch: 34, Batch:   100, Loss: 10.762\n",
            "Epoch: 34, Batch:   200, Loss: 10.777\n",
            "Epoch: 34, Batch:   300, Loss: 10.773\n",
            "Epoch [34/50], Loss: 290.6139802932739, Accuracy: 0.11219418960244648\n",
            "Epoch: 35, Batch:   100, Loss: 10.733\n",
            "Epoch: 35, Batch:   200, Loss: 10.749\n",
            "Epoch: 35, Batch:   300, Loss: 10.733\n",
            "Epoch [35/50], Loss: 291.3160409927368, Accuracy: 0.11439220183486239\n",
            "Epoch: 36, Batch:   100, Loss: 10.732\n",
            "Epoch: 36, Batch:   200, Loss: 10.713\n",
            "Epoch: 36, Batch:   300, Loss: 10.703\n",
            "Epoch [36/50], Loss: 288.66435050964355, Accuracy: 0.1128631498470948\n",
            "Epoch: 37, Batch:   100, Loss: 10.699\n",
            "Epoch: 37, Batch:   200, Loss: 10.706\n",
            "Epoch: 37, Batch:   300, Loss: 10.698\n",
            "Epoch [37/50], Loss: 288.4519681930542, Accuracy: 0.11639908256880734\n",
            "Epoch: 38, Batch:   100, Loss: 10.666\n",
            "Epoch: 38, Batch:   200, Loss: 10.673\n",
            "Epoch: 38, Batch:   300, Loss: 10.699\n",
            "Epoch [38/50], Loss: 288.90761852264404, Accuracy: 0.11716360856269113\n",
            "Epoch: 39, Batch:   100, Loss: 10.652\n",
            "Epoch: 39, Batch:   200, Loss: 10.661\n",
            "Epoch: 39, Batch:   300, Loss: 10.666\n",
            "Epoch [39/50], Loss: 287.8583059310913, Accuracy: 0.11606460244648319\n",
            "Epoch: 40, Batch:   100, Loss: 10.636\n",
            "Epoch: 40, Batch:   200, Loss: 10.620\n",
            "Epoch: 40, Batch:   300, Loss: 10.639\n",
            "Epoch [40/50], Loss: 286.97440910339355, Accuracy: 0.111190749235474\n",
            "Epoch: 41, Batch:   100, Loss: 10.631\n",
            "Epoch: 41, Batch:   200, Loss: 10.616\n",
            "Epoch: 41, Batch:   300, Loss: 10.636\n",
            "Epoch [41/50], Loss: 286.00055027008057, Accuracy: 0.11663799694189603\n",
            "Epoch: 42, Batch:   100, Loss: 10.607\n",
            "Epoch: 42, Batch:   200, Loss: 10.606\n",
            "Epoch: 42, Batch:   300, Loss: 10.594\n",
            "Epoch [42/50], Loss: 287.45813941955566, Accuracy: 0.11730695718654434\n",
            "Epoch: 43, Batch:   100, Loss: 10.589\n",
            "Epoch: 43, Batch:   200, Loss: 10.583\n",
            "Epoch: 43, Batch:   300, Loss: 10.570\n",
            "Epoch [43/50], Loss: 284.8851728439331, Accuracy: 0.11305428134556575\n",
            "Epoch: 44, Batch:   100, Loss: 10.555\n",
            "Epoch: 44, Batch:   200, Loss: 10.569\n",
            "Epoch: 44, Batch:   300, Loss: 10.572\n",
            "Epoch [44/50], Loss: 285.96079540252686, Accuracy: 0.11950496941896024\n",
            "Epoch: 45, Batch:   100, Loss: 10.548\n",
            "Epoch: 45, Batch:   200, Loss: 10.531\n",
            "Epoch: 45, Batch:   300, Loss: 10.563\n",
            "Epoch [45/50], Loss: 284.3360013961792, Accuracy: 0.11773700305810397\n",
            "Epoch: 46, Batch:   100, Loss: 10.544\n",
            "Epoch: 46, Batch:   200, Loss: 10.544\n",
            "Epoch: 46, Batch:   300, Loss: 10.535\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-538276cb7a4f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# adversarial_loss_fn = nn.CrossEntropyLoss()  # Use Cross Entropy Loss for multi-class classification\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     running_loss = 0.0\n",
        "#     total_correct = 0\n",
        "#     total_samples = 0\n",
        "#     for i, data in enumerate(train_loader, 0):\n",
        "#         inputs, labels = data\n",
        "#         inputs = inputs.permute(0, 3, 1, 2).float().to(device)  # Move inputs to GPU\n",
        "#         labels = labels.float().to(device)  # Move labels to GPU\n",
        "#         # print(labels.shape)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "#         # print(outputs.shape)\n",
        "#         # Calculate adversarial loss\n",
        "#         adversarial_loss = adversarial_loss_fn(outputs, labels)\n",
        "\n",
        "#         b_loss = 0 #boundary_loss_fn(outputs, labels)\n",
        "#         total_loss = weight_adversarial * adversarial_loss + weight_boundary * b_loss\n",
        "#         # Apply gradient clipping\n",
        "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "#         total_loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += total_loss.item()\n",
        "\n",
        "#         if i % 100 == 99:\n",
        "#             print('Epoch: %d, Batch: %5d, Loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "#             running_loss = 0.0\n",
        "\n",
        "#         # Calculate accuracy\n",
        "#         _, predicted_indices = torch.max(outputs, 1)  # Get the index with the highest probability\n",
        "#         # print(predicted_indices.shape)\n",
        "#         # print(labels.shape)\n",
        "#         correct = (predicted_indices == labels.argmax(dim=1)).sum().item()\n",
        "#         total_correct += correct\n",
        "#         total_samples += labels.size(0)\n",
        "\n",
        "#     accuracy = total_correct / total_samples\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}, Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "JigsawModel(\n",
              "  (conv1): Conv2d(27, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=12800, out_features=4096, bias=True)\n",
              "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc4): Linear(in_features=512, out_features=81, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ubqLHdhvM1rl"
      },
      "outputs": [],
      "source": [
        "def recur(sequence, outputs):\n",
        "    sequence = np.array(sequence)\n",
        "    for i in range(len(sequence)):\n",
        "        while np.sum(sequence == sequence[i]) > 1:  # If the current element is a duplicate\n",
        "            # Find the index in outputs for the current sequence element that is not yet in the updated sequence\n",
        "            scores = outputs[i]\n",
        "            sorted_indices = np.argsort(scores)[::-1]  # Indices of scores sorted in descending order\n",
        "            for idx in sorted_indices:\n",
        "                if idx not in sequence:\n",
        "                    sequence[i] = idx\n",
        "                    break\n",
        "\n",
        "    return sequence.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGPwcL4sM1rm",
        "outputId": "2b61cbd9-38c5-424c-a619-cbafcdab412e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test images: 3 %\n",
            "Per tile accuracy on test images: 40 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "per_tile_accuracy = 0\n",
        "################\n",
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.permute(0, 3, 1, 2).float()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # reshape the output to 9x9 matrix\n",
        "        outputs = outputs.reshape(-1, 9, 9)\n",
        "        predicted = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # now doing the same for the target\n",
        "        labels = labels.reshape(-1, 9, 9)\n",
        "        target = torch.argmax(labels, dim=1)\n",
        "\n",
        "        # check if the predicted sequence is correct\n",
        "        for i in range(len(predicted)):\n",
        "            updated_predicted = recur(predicted[i], outputs[i, : , :].numpy())\n",
        "\n",
        "            if torch.equal(torch.tensor(updated_predicted), target[i]):\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "            per_tile_accuracy += (np.array(updated_predicted) == target[i].numpy()).sum() / 9\n",
        "\n",
        "print('Accuracy on test images: %d %%' % (100 * correct / total))\n",
        "print('Per tile accuracy on test images: %d %%' % (100 * per_tile_accuracy / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMuIZCq3M1rm"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# # Save the model\n",
        "# torch.save(model.state_dict(), 'saved_models/bloss_model_38.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmNZx3oSkuaG",
        "outputId": "c90cf628-659b-459e-b410-f1db0f5be985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Specify the directory where you want to save the model\n",
        "# save_dir = '/content/drive/My Drive/saved_models/'\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# # Save the model\n",
        "# torch.save(model.state_dict(), os.path.join(save_dir, 'bloss_model_38_50.pth'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mcmzzsnCM1rn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-2.5402e+00, -1.6388e+00, -2.3079e+00, -3.0343e+00, -2.2356e+00,\n",
            "          4.6958e-01, -4.1293e+00, -1.4466e+00,  5.9697e-01],\n",
            "        [-1.6023e+00, -2.6041e+00, -4.5954e-01, -1.2689e+00, -2.2203e+00,\n",
            "         -3.1179e-01,  2.7006e-01, -2.7077e+00,  1.4654e+00],\n",
            "        [-3.2233e+00, -6.1245e+00, -3.7110e+00, -3.7112e-01,  2.7203e+00,\n",
            "         -1.5710e+00, -1.5550e+00, -2.3751e+00, -9.8417e-01],\n",
            "        [-1.4831e+00,  1.2965e+00,  1.8673e+00, -6.8744e+00, -6.3272e+00,\n",
            "         -5.9607e+00, -8.7382e+00, -1.4744e+01, -1.2202e+01],\n",
            "        [ 7.6922e-01,  1.8393e+00, -1.2242e+00, -2.6569e+00, -9.7774e+00,\n",
            "         -7.0908e+00, -6.5684e+00, -1.2753e+01, -9.3459e+00],\n",
            "        [-7.6305e-01, -4.0106e+00, -4.5658e+00,  9.3758e-01, -5.0661e-01,\n",
            "         -3.9638e+00, -3.9317e+00, -2.3472e+00, -5.6322e+00],\n",
            "        [-5.1704e+00, -7.8025e+00, -3.2860e+00, -4.0859e+00, -2.8290e+00,\n",
            "          8.3692e-03, -2.5862e+00,  3.3608e-01, -3.5263e-01],\n",
            "        [-7.1278e+00, -6.4387e+00, -6.3696e+00, -2.0419e+00, -2.6101e+00,\n",
            "         -3.8866e+00,  1.6407e+00,  2.0773e+00, -2.1273e+00],\n",
            "        [-3.0583e+00, -5.3577e+00, -2.7372e+00, -2.2583e+00, -4.7216e-01,\n",
            "          1.0325e+00, -1.6256e+00, -1.3611e+00, -1.0278e+00]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "[0 6 4 2 1 3 8 7 5]\n"
          ]
        }
      ],
      "source": [
        "idx = random.randint(0, len(X_test))\n",
        "image = X_test[idx]\n",
        "target = y_test[idx]\n",
        "inputs = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
        "outputs = model(inputs)\n",
        "outputs = outputs.reshape(9, 9)\n",
        "print(outputs)\n",
        "converted_target = np.argmax(target.reshape(9, 9), axis=1)\n",
        "print(converted_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recur(sequence, outputs):\n",
        "    sequence = np.array(sequence)\n",
        "    for i in range(len(sequence)):\n",
        "        while np.sum(sequence == sequence[i]) > 1:  # If the current element is a duplicate\n",
        "            # Find the index in outputs for the current sequence element that is not yet in the updated sequence\n",
        "            scores = outputs[i]\n",
        "            sorted_indices = np.argsort(scores)[::-1]  # Indices of scores sorted in descending order\n",
        "            for idx in sorted_indices:\n",
        "                if idx not in sequence:\n",
        "                    sequence[i] = idx\n",
        "                    break\n",
        "                \n",
        "    return sequence.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stitch_shuffled_image(parts):\n",
        "    num_parts = len(parts)\n",
        "    part_size = parts[0].shape[0]  # Assuming all parts are square\n",
        "    \n",
        "    stitched_image_size = int(np.sqrt(num_parts) * part_size)\n",
        "    stitched_image = np.zeros((stitched_image_size, stitched_image_size, parts[0].shape[2]), dtype=np.uint8)\n",
        "    \n",
        "    for i in range(stitched_image.shape[0] // part_size):\n",
        "        for j in range(stitched_image.shape[1] // part_size):\n",
        "            part_index = i * int(stitched_image.shape[0] / part_size) + j\n",
        "            stitched_image[i*part_size:(i+1)*part_size, j*part_size:(j+1)*part_size] = parts[part_index]\n",
        "    \n",
        "    return stitched_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uRV7vzp8M1rn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cv2\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Assuming reconstructed_image and recur functions are defined elsewhere\n",
        "\n",
        "# Select random image from test set\n",
        "idx = random.randint(0, len(X_test))\n",
        "image = X_test[idx]\n",
        "target = y_test[idx]\n",
        "\n",
        "# Plot only the reconstructed image with predicted sequence\n",
        "inputs = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float()\n",
        "outputs = model(inputs)\n",
        "outputs = outputs.reshape(9, 9)\n",
        "\n",
        "predicted = torch.argmax(outputs, dim=1)\n",
        "updated_predicted = recur(predicted, outputs.detach().numpy())\n",
        "\n",
        "reconstructed_img = reconstructed_image(image, updated_predicted)\n",
        "\n",
        "# Convert the image to BGR format as cv2 uses BGR by default\n",
        "reconstructed_img_bgr = cv2.cvtColor(reconstructed_img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "# Save the reconstructed image with the predicted sequence\n",
        "cv2.imwrite('reconstructed_image_with_predicted_sequence_5.png', reconstructed_img_bgr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
